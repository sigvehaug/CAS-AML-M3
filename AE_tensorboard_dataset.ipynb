{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE_tensorboard_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TVG7kpjf8T2D",
        "m3xcUuod8bUt",
        "7-YdzZ2QDwLg",
        "4saA2FcDFcMq",
        "tu5h3HaT6aCN",
        "AVUXN5Jq7kx9",
        "9qArUdx6YHzD",
        "Tk1sA3cNFuLB",
        "phTJlLGx3a5H",
        "8_tWAYpah3aG",
        "9PLAxq3XIho_",
        "lgVvSoXa5vOk",
        "G3DrSyps5wWv",
        "kHPnzzd_6PLO",
        "YVruUa0d7fMw",
        "G1_R6CnAy7-F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smjlo1x48N3t"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6H2W9QmdP0U"
      },
      "source": [
        "<p>\n",
        "CAS on Advanced Machine Learning 2021<br>\n",
        "Prepared by Dr. Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVG7kpjf8T2D"
      },
      "source": [
        "\n",
        "## Libs and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdEp_bJqBWYL"
      },
      "source": [
        "from matplotlib import  pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from time import time as timer\n",
        "import requests\n",
        "import zipfile\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.datasets.mnist as mnist\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, Flatten, Input, Reshape, Cropping2D, Embedding\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Audio\n",
        "import IPython\n",
        "\n",
        "\n",
        "#import umap\n",
        "from scipy.stats import entropy\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF3Aeyzx8sEW"
      },
      "source": [
        "# merging 2d matrix of images in 1 image\n",
        "def mosaic(mtr_of_ims):\n",
        "  ny = len(mtr_of_ims)\n",
        "  assert(ny != 0)\n",
        "\n",
        "  nx = len(mtr_of_ims[0])\n",
        "  assert(nx != 0)\n",
        "\n",
        "  im_sh = mtr_of_ims[0][0].shape\n",
        "\n",
        "  assert (2 <= len(im_sh) <= 3)\n",
        "  multichannel = len(im_sh) == 3\n",
        "\n",
        "  if multichannel:\n",
        "    h, w, c = im_sh\n",
        "  else:\n",
        "    h, w = im_sh\n",
        "\n",
        "  h_c = h * ny + 1 * (ny-1)\n",
        "  w_c = w * nx + 1 * (nx-1)\n",
        "\n",
        "  canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
        "  canvas = np.ones(shape=canv_sh, dtype=np.float32)*0.5\n",
        "\n",
        "  for iy, row in enumerate(mtr_of_ims):\n",
        "    y_ofs = iy * (h + 1)\n",
        "    for ix, im in enumerate(row):\n",
        "      x_ofs = ix * (w + 1)\n",
        "      canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
        "\n",
        "\n",
        "  return canvas  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3xcUuod8bUt"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWqy2uMffPA"
      },
      "source": [
        "Lets start with a simple, well understood mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JDPbgcBBoUd"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj_UTkRS-Oc1"
      },
      "source": [
        "# normalize images in [0,1]\n",
        "train_images = (train_images/255).astype(np.float32)\n",
        "test_images = (test_images/255).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbR9emfD8d41"
      },
      "source": [
        "# Helper Autoencoder Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNM2dEZOh5EN"
      },
      "source": [
        "We will start from implementing an Autoencoder model base class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VWnyViGsM_o"
      },
      "source": [
        "class AE(tf.keras.Model):\n",
        "  def __init__(self, in_size, n_code, noise_rate=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.data_size = list(in_size)  # shape of data sample\n",
        "    self.flat_data_size = np.prod(self.data_size)\n",
        "    \n",
        "    self.noise_rate = min(0.99, max(0, noise_rate)) # noise rate for denoising AE\n",
        "    self.denoising = self.noise_rate != 0\n",
        "\n",
        "    self.x_d = None  # variable to keep the input data\n",
        "    self.xn_d = None # input data with added noise (for DAE)\n",
        "\n",
        "    self.x_d_val = None  # validation dataset \n",
        "    self.c_d_val = None  # class labels for validation dataset\n",
        "\n",
        "    self.history = {} # training history\n",
        "    self.sample_history = {}  # history of validation sample evolution in latent space and reconstruction\n",
        "    self.weights_history = {} # history of model weights (joint model can't be saved at the moment)\n",
        "\n",
        "    self.out = display(IPython.display.Pretty(''), display_id=True)\n",
        "\n",
        "    self.last_n_ep = 0  # number of epochs of last fit run\n",
        "\n",
        "    self.n_code = n_code # number of latent dimensions\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    \n",
        "    self.create()\n",
        "\n",
        "  def create(self):\n",
        "    \"\"\"\n",
        "    Here the model is built\n",
        "    \"\"\"\n",
        "\n",
        "    # encoder model\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.data_size),\n",
        "         Flatten(),\n",
        "         Dense(128, activation='relu', kernel_initializer='he_normal', name='e_l1'),\n",
        "         Dense(self.n_code, activation='sigmoid', kernel_initializer='he_normal', name='e_l2'),\n",
        "        ])\n",
        "    \n",
        "    #decoder model\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.n_code),\n",
        "         Dense(128, activation='relu', kernel_initializer='he_normal', name='d_l1'),\n",
        "         Dense(self.flat_data_size, activation='sigmoid', kernel_initializer='he_normal', name='d_l2'),\n",
        "         Reshape(target_shape=self.data_size,),\n",
        "        ])\n",
        "         \n",
        "\n",
        "    # build the model\n",
        "    self.compile(\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "          loss=tf.keras.losses.MeanSquaredError(),\n",
        "      )\n",
        "      \n",
        "  def encode(self, x):\n",
        "    z = self.encoder(x)\n",
        "    return z\n",
        "\n",
        "  def decode(self, z):\n",
        "    y = self.decoder(z)\n",
        "    return y\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"forward pass\"\"\"\n",
        "    z = self.encoder(x)\n",
        "    y = self.decoder(z)\n",
        "    return y\n",
        "\n",
        "  class EvalNSamples(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback class for evaluating ans saving latent state and prediciton for validation samples\n",
        "    \"\"\"\n",
        "    def __init__(self, ae, n=32):\n",
        "      super().__init__()\n",
        "      self.ae = ae\n",
        "      self.n_sampl = n\n",
        "\n",
        "      self.ims_smpl = None\n",
        "      self.lbls_smpl = None\n",
        "\n",
        "      self.get_uniform_subsample()\n",
        "\n",
        "    def get_uniform_subsample(self):\n",
        "      \"\"\"\n",
        "      Get self.n_sampl elements for each of the classes\n",
        "      for latent space evolution\n",
        "      \"\"\"\n",
        "      ims = []\n",
        "      lbls = []\n",
        "      for class_idx in range(np.max(self.ae.c_d_val)+1):\n",
        "        map_d = self.ae.c_d_val == class_idx\n",
        "        ims_d = self.ae.x_d_val[map_d]\n",
        "\n",
        "        smpl_idx = np.random.choice(len(ims_d), self.n_sampl)\n",
        "        ims_d_smpl = ims_d[smpl_idx]\n",
        "        \n",
        "        ims.append(ims_d_smpl)\n",
        "        lbls.append([class_idx]*self.n_sampl)\n",
        "\n",
        "      self.ims_smpl = np.concatenate(ims)\n",
        "      self.lbls_smpl = np.concatenate(lbls)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        samples = self.ae.x_d_val[:self.n_sampl]\n",
        "        labels = self.ae.c_d_val[:self.n_sampl]\n",
        "        res = {'x': samples, 'l': labels}\n",
        "        if self.ae.denoising:\n",
        "          samples = self.ae.add_noise(samples)\n",
        "          res['xn'] = samples\n",
        "\n",
        "        res['y'] = self.ae.predict(samples)\n",
        "        res['z'] = self.ae.encoder(samples).numpy()\n",
        "\n",
        "        res['l_unif'] = self.lbls_smpl\n",
        "        res['z_unif'] = self.ae.encoder(self.ims_smpl).numpy()\n",
        "\n",
        "        self.ae.sample_history[epoch] = res   \n",
        "        #keys = list(logs.keys())\n",
        "        #print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "  class SaveAE(tf.keras.callbacks.Callback):\n",
        "      \"\"\"\n",
        "      Callback class for saving model weights along training\n",
        "      \"\"\"\n",
        "      def __init__(self, ae):\n",
        "        super().__init__()\n",
        "        self.ae = ae\n",
        "\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "        weights_encoder = self.ae.encoder.get_weights()\n",
        "        weights_decoder = self.ae.decoder.get_weights()\n",
        "\n",
        "        self.ae.weights_history[epoch] = {\n",
        "            'w_encoder': weights_encoder,\n",
        "            'w_decoder': weights_decoder,\n",
        "        }   \n",
        "\n",
        "  def _fit(self, x, y=None, epochs=None, batch_size=None,\n",
        "           validation_data=None, callbacks=None):\n",
        "    \"\"\"\n",
        "    Here actual model fitting is performed.\n",
        "    Can be reimplemented in inherited class for custom training loop (needed for VAE)\n",
        "    \"\"\"\n",
        "    if y is None:\n",
        "          return super().fit(x=x,\n",
        "                       epochs=epochs,\n",
        "                       validation_data=validation_data,\n",
        "                       callbacks=callbacks)\n",
        "  \n",
        "    else:\n",
        "      return super().fit(x=x, y=y,\n",
        "                        epochs=epochs, batch_size=batch_size, \n",
        "                        validation_data=validation_data,\n",
        "                        callbacks=callbacks)\n",
        "  \n",
        "  def fit(self, training_data, n_epochs, \n",
        "          validation_data=None, lr=None, \n",
        "          batch_size = 64,\n",
        "          epoch_callback=None,\n",
        "          callbacks=None\n",
        "          ):\n",
        "    \"\"\"\n",
        "    Interface for model training\n",
        "    Incapsulates all the callbacks, adding noise to training data etc\n",
        "    \"\"\"\n",
        "\n",
        "    t0 = timer()\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(training_data)\n",
        "    train_dataset = train_dataset.map(lambda x: (x, self.add_noise(x)))\n",
        "    train_dataset = train_dataset.shuffle(60000)\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    train_dataset = train_dataset.prefetch(5)\n",
        "\n",
        "    self.x_d = training_data\n",
        "    self.x_d_val, self.c_d_val = validation_data\n",
        "    \n",
        "    #self.xn_d = self.add_noise(self.x_d)\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    callbacks = callbacks or []\n",
        "    callbacks = [AE.EvalNSamples(self), AE.SaveAE(self), tensorboard_callback]\n",
        "    # if save_dir:\n",
        "    #   callbacks += [save_callback]\n",
        "    \n",
        "    if lr is not None:\n",
        "      self.optimizer.lr.assign(lr)\n",
        "\n",
        "    self.history = self._fit(train_dataset,\n",
        "                                  epochs=n_epochs, \n",
        "                                  validation_data=(self.x_d_val, \n",
        "                                                   self.x_d_val),\n",
        "                                  callbacks=callbacks)\n",
        "                \n",
        "    self.last_n_ep = n_epochs\n",
        "    t1 = timer()\n",
        "    self.print(f'fit time {t1-t0:.0f} sec')\n",
        "\n",
        "  def add_noise(self, x):\n",
        "    \"\"\"\n",
        "    Adds Salt&Pepper nois to imput data.\n",
        "    Currently noisy samples are generated only once, not for each epoch.\n",
        "    \"\"\"\n",
        "    \n",
        "    if self.denoising:\n",
        "      sh = x.shape \n",
        "      \n",
        "      noise_mask = np.random.binomial(n=1, p=self.noise_rate, size=sh)\n",
        "      sp_noise = np.random.binomial(n=1, p=0.5, size=sh)\n",
        "\n",
        "      x = x * (1-noise_mask) + sp_noise * noise_mask\n",
        "\n",
        "      self.sp = sp_noise\n",
        "\n",
        "    return x\n",
        "\n",
        "  def print(self, msg):\n",
        "    self.out.update(IPython.display.Pretty(msg))\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    self.decoder.summary()\n",
        "\n",
        "  def plot_hist(self):\n",
        "    \"\"\"\n",
        "    plot training loss\n",
        "    \"\"\"\n",
        "    hist = self.history.history\n",
        "    if not hist:\n",
        "      self.print('run `fit` first to train the model')\n",
        "      return\n",
        "\n",
        "    loss = hist['loss']\n",
        "    v_loss = hist['val_loss']\n",
        "    eps = np.arange(len(loss))\n",
        "    plt.semilogy(eps, loss, label='training');\n",
        "    if 'val_loss' in hist:\n",
        "      plt.semilogy(eps, v_loss, label='validation');\n",
        "    plt.legend()\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "  def plot_samples(self, stride=5, fig_scale=1):\n",
        "    \"\"\"\n",
        "    Plots input, noisy samples (for DAE) and reconstruction.\n",
        "    Each `stride`-th epoch\n",
        "    \"\"\"\n",
        "\n",
        "    hist = self.sample_history\n",
        "    for epoch_idx, hist_el in hist.items():\n",
        "      if epoch_idx % stride != 0 and epoch_idx != np.max(list(hist.keys())):\n",
        "        continue\n",
        "        \n",
        "      samples = []\n",
        "      for k, els in hist_el.items():\n",
        "        if k not in ['x', 'xn', 'y']:\n",
        "          continue\n",
        "        samples.append(els)\n",
        "\n",
        "      ny = len(samples)\n",
        "      nx = len(samples[0])\n",
        "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
        "      m = mosaic(samples)\n",
        "      plt.title(f'after epoch {int(epoch_idx)}')\n",
        "      plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "      plt.tight_layout(0.1, 0, 0)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "  def run_on_trained(self, run_fn, ep=None):\n",
        "    \"\"\"\n",
        "    Helper funcrion to excecute any function on model in state after `ep` training epoch\n",
        "    \"\"\"\n",
        "    ep = ep if (ep is not None) else (self.last_n_ep-1)\n",
        "    self.encoder.set_weights(self.weights_history[ep]['w_encoder'])\n",
        "    self.decoder.set_weights(self.weights_history[ep]['w_decoder'])\n",
        "    \n",
        "    run_fn(self)\n",
        "\n",
        "  def run_on_all_training_history(self, run_fn, n_ep=None):\n",
        "    \"\"\"\n",
        "    Helper funcrion to excecute any function on model state after each of the training epochs\n",
        "    \"\"\"\n",
        "    n_ep = n_ep if (n_ep is not None) else (self.last_n_ep)\n",
        "    for ep in range(n_ep):\n",
        "      self.print(f'running on epoch {ep+1}/{n_ep}...')\n",
        "      self.run_on_trained(run_fn, ep)\n",
        "    self.print(f'done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QH3J3Z-jTb2"
      },
      "source": [
        "# Simple Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_tJ_N1tmy36"
      },
      "source": [
        "First we train create an autoencoder with 5 latent variables for MNIST dataset.\n",
        "Each image is 28x28 pixels.\n",
        "We start from image data since it's easy to interpret visually, but the very same applies to other data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQFSyzXNO6Qj"
      },
      "source": [
        "ae = AE((28,28), n_code=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z0HUosxI5pD"
      },
      "source": [
        "ae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoE4LQB1nFWc"
      },
      "source": [
        "Train the model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnMmXhj1Dwhx"
      },
      "source": [
        "%rm -rf logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFw9Hmj2JKWe"
      },
      "source": [
        "ae.fit(train_images, 50, validation_data=(test_images, test_labels), lr=0.0009, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTj3XOMS_8TE"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQMa3LZpnS_c"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud71UyCRJRF0"
      },
      "source": [
        "ae.plot_hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSh4NLrjtLqs"
      },
      "source": [
        "Let's visually compare network's output with the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbQI57w6SFw1"
      },
      "source": [
        "ae.plot_samples()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-YdzZ2QDwLg"
      },
      "source": [
        "# Denoising Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15yh2Cfvh9RZ"
      },
      "source": [
        "In denoising AE we create noisy samples and ask the model to reconstruct original (clean) samples. Here 20% of pixels will be corrupted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f2cXxY_D6hN"
      },
      "source": [
        "dae = AE((28,28), noise_rate=0.2, n_code=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evgQqWGMD6hW"
      },
      "source": [
        "dae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_7jcNtIozy7"
      },
      "source": [
        "Train same way as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl5zb0JZD6hZ"
      },
      "source": [
        "dae.fit(train_images, 50, validation_data=(test_images, test_labels), lr=0.0009, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6bBd4JSo4-e"
      },
      "source": [
        "Plot loss history:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpDps1FXD6hc"
      },
      "source": [
        "dae.plot_hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE4x3ebDo9Or"
      },
      "source": [
        "Visualise evolution of reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbNOqMWbc5--"
      },
      "source": [
        "dae.plot_samples()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4saA2FcDFcMq"
      },
      "source": [
        "# Exercise 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZpzOK_OtsUu"
      },
      "source": [
        "Use autoencoder to find outliers:\n",
        "1. preform reconstruciton for test dataset\n",
        "2. evaluate reconstruction loss and plot distrubution\n",
        "3. visualise poorely reconstructed samples\n",
        "\n",
        "Homework:\n",
        "Perform 2D UMAP embedding of the raw data points and image encoding (variables in latent space) for test set, plot it and highlight outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu5h3HaT6aCN"
      },
      "source": [
        "# Convolutional autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf1wpycv6djD"
      },
      "source": [
        "Instead of fully connected layers we can use strided convolutional layers in encoder, and transposed convolutions in decoder.\n",
        "This model will have less parameters due to weight sharing, thus easier to train.\n",
        "\n",
        "After upscaling the image size will be a bit bigger then original, so we also crop reconstruction to the input image size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nXZo6a2PJAu"
      },
      "source": [
        "class CAE(AE):\n",
        "  def __init__(self, in_size, n_code, noise_rate=0):\n",
        "    super().__init__(in_size, n_code, noise_rate)\n",
        "    self.preflat_shape = 1\n",
        "\n",
        "  def create(self):\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.data_size),\n",
        "         Reshape(target_shape = self.data_size+[1]),\n",
        "         Conv2D(8, 3, padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2D(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2D(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2D(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Flatten(),\n",
        "         Dense(self.n_code, activation='sigmoid', kernel_initializer='he_normal')\n",
        "        ])\n",
        "    \n",
        "    self.preflat_shape = self.encoder.layers[-2].input.get_shape().as_list()[1:]\n",
        "    \n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.n_code),\n",
        "         Dense(np.prod(self.preflat_shape), activation='relu', kernel_initializer='he_normal'),\n",
        "         Reshape(target_shape=self.preflat_shape),\n",
        "         Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(8, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2D(1, 3, strides=(1,1), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Cropping2D(cropping=((2, 2), (2, 2))),\n",
        "         Reshape(target_shape=self.data_size,),\n",
        "        ])\n",
        "    \n",
        "    h, w = self.decoder.layers[-1].output.get_shape().as_list()[1:3]  # reconstructed width and hight\n",
        "    h_tgt, w_tgt = self.data_size[:2]  # target width and height\n",
        "    dh = h - h_tgt  # deltas to be cropped away\n",
        "    dw = w - w_tgt\n",
        "\n",
        "    # add to decoder cropping layer and final reshaping\n",
        "    self.decoder.add(Cropping2D(cropping=((dh//2, dh-dh//2), (dw//2, dw-dw//2))))\n",
        "    self.decoder.add(Reshape(target_shape=self.data_size,))\n",
        "         \n",
        "    self.compile(\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "          loss=tf.keras.losses.MeanSquaredError(),\n",
        "      )\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    return super().call(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ2L-doCtkNc"
      },
      "source": [
        "Thus we can try to reduce sise of the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v28YH6hRbvQ"
      },
      "source": [
        "cae = CAE((28,28), noise_rate=0.2, n_code=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9V7C7OsP0Ks"
      },
      "source": [
        "cae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzRdn1wwP0Kw"
      },
      "source": [
        "cae.fit(train_images, 20, validation_data=(test_images, test_labels), lr=0.0018, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFXNLlTbkX1Y"
      },
      "source": [
        "cae.plot_hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NuwCAE77VvV"
      },
      "source": [
        "cae.plot_samples()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVUXN5Jq7kx9"
      },
      "source": [
        "# Latent space\n",
        "\n",
        "So far we looked just on the output: the model sort of does the job. But what does it learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mVBP8b0t-Wl"
      },
      "source": [
        "first let's use animation to visualize reconstruction evolution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wzDeCVObTlI"
      },
      "source": [
        "hist = cae.sample_history\n",
        "\n",
        "smpl_epochs = list(hist.keys())\n",
        "\n",
        "smpl_ims = [[hist[ep]['x'], hist[ep]['y']] for ep in smpl_epochs]\n",
        "\n",
        "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "\n",
        "s=1\n",
        "fig = plt.figure(figsize=(s*nx, s*ny))\n",
        "\n",
        "m = mosaic(smpl_ims[0])\n",
        "\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    m = mosaic(smpl_ims[i])\n",
        "    imsh.set_data(m)\n",
        "\n",
        "    ttl = plt.title(f'after epoch {i}')\n",
        "\n",
        "    return imsh\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))\n",
        "\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qu6J9kGuEt9"
      },
      "source": [
        "And let's see evolution of the latent representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngMUudwdDpg5"
      },
      "source": [
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "hist = cae.sample_history\n",
        "\n",
        "smpl_epochs = list(hist.keys())\n",
        "\n",
        "z_res = [hist[ep]['z_unif'] for ep in smpl_epochs]\n",
        "l_res = [hist[ep]['l_unif'] for ep in smpl_epochs]\n",
        "\n",
        "\n",
        "scat = plt.scatter(z_res[0][:,0], z_res[0][:,1], c=l_res[0], cmap=cm.rainbow)\n",
        "plt.xlim(-0.1, 1.1)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "\n",
        "legend1 = plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
        "plt.gca().add_artist(legend1)\n",
        "plt.gca().set_aspect('equal')\n",
        "ttl = plt.title(f'after epoch {0}')\n",
        "\n",
        "def animate(i):\n",
        "    z = z_res[i]\n",
        "    scat.set_offsets(z)\n",
        "    ttl = plt.title(f'after epoch {i}')\n",
        "    return scat\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))\n",
        "\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qArUdx6YHzD"
      },
      "source": [
        "# Sampling from latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syOBR27sn6vm"
      },
      "source": [
        "But we can also use the trained model to generate samples based on the latent representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLRUV7ad5P8p"
      },
      "source": [
        "z0 = np.random.uniform(size=(20, 2))\n",
        "\n",
        "ims_all = []\n",
        "def fn(ae):\n",
        "  ims = ae.decode(z0)\n",
        "  ims_all.append(ims)\n",
        "\n",
        "cae.run_on_all_training_history(fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edg10WV74629"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 1.5))\n",
        "\n",
        "m = mosaic([ims_all[0]])\n",
        "\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    m = mosaic([ims_all[i]])\n",
        "    imsh.set_data(m)\n",
        "    ttl = plt.title(f'after epoch {i}')\n",
        "    return imsh\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(ims_all))\n",
        "\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk1sA3cNFuLB"
      },
      "source": [
        "# Interpolation in latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsyVm5m_F3ng"
      },
      "source": [
        "zs = np.meshgrid(np.linspace(0, 1, 10),\n",
        "                 np.linspace(0, 1, 10))\n",
        "zs = np.stack(zs, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHivvVP0KaQd"
      },
      "source": [
        "def fn(ae):\n",
        "  ims = ae.decode(zs.reshape((-1, 2))).numpy()\n",
        "  sh = list(ims.shape)\n",
        "  ims = ims.reshape([10, 10]+sh[1:])\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "cae.run_on_trained(fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phTJlLGx3a5H"
      },
      "source": [
        "# Exercise 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXr6-NGf6OW4"
      },
      "source": [
        "\n",
        "Option 1. Compare distribution in latent space for noise rate 0, 0.2, 0.6, 0.8.\n",
        "\n",
        "Option 2. Compare reconstruction depending on size of latent space: 2, 4, 8, 16, 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_tWAYpah3aG"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnBr9bvW39zT"
      },
      "source": [
        "While the previous model learn to seaparate subpopulations in the latent space, there remains significant overlap & unpopulated regions.\n",
        "\n",
        "Varitional AE puts additional constraints on the distribution in the latent space and perform variational inference.\n",
        "\n",
        "(see pptx for details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kshKMIOwwD9W"
      },
      "source": [
        "Here the encoder will predict `2 x n_code` values: means and logarithm of variance for each sample. Since these values live in $R^2$ - no activation function is used in last layer of the encoder.\n",
        "\n",
        "Then for reconstruction we will sample from this distribuition with a reparametrisation trick.\n",
        "\n",
        "The tecnically complex part - implement custom loss function and training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU2h-B6fVZYt"
      },
      "source": [
        "class VCAE(AE):\n",
        "  def __init__(self, in_size, n_code, noise_rate=0):\n",
        "    super().__init__(in_size, n_code, noise_rate)\n",
        "    self.preflat_shape = 1\n",
        "\n",
        "  def create(self):\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.data_size),\n",
        "         Reshape(target_shape = self.data_size+[1]),\n",
        "         Conv2D(16, 3,                padding='same', activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Conv2D(16, 3, strides=(2,2), padding='same', activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Conv2D(32, 3, strides=(2,2), padding='same', activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Conv2D(32, 3, strides=(2,2), padding='same', activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Conv2D(32, 3, strides=(2,2), padding='same', activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Conv2D(64, 3, strides=(2,2), padding='same', activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Flatten(),\n",
        "         Dense(64, activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Dense(self.n_code * 2, kernel_initializer='he_normal')\n",
        "        ])\n",
        "    \n",
        "    self.preflat_shape = self.encoder.layers[-3].input.get_shape().as_list()[1:]\n",
        "    \n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.n_code),\n",
        "         Dense(64,                          activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Dense(np.prod(self.preflat_shape), activation=LeakyReLU(alpha=0.3), kernel_initializer='he_normal'),\n",
        "         Reshape(target_shape=self.preflat_shape),\n",
        "         Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
        "         Conv2D(1, 3, strides=(1,1), padding='same', activation='sigmoid', kernel_initializer='he_normal'),\n",
        "        ])\n",
        "    \n",
        "    h, w = self.decoder.layers[-1].output.get_shape().as_list()[1:3]\n",
        "    h_tgt, w_tgt = self.data_size[:2]\n",
        "    dh = h - h_tgt\n",
        "    dw = w - w_tgt\n",
        "\n",
        "    self.decoder.add(Cropping2D(cropping=((dh//2, dh-dh//2), (dw//2, dw-dw//2))))\n",
        "    self.decoder.add(Reshape(target_shape=self.data_size,))\n",
        "         \n",
        "    self.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    \n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.n_code))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    z_mean, z_logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return z_mean, z_logvar\n",
        "   \n",
        "  def reparameterize(self, z_mean, z_logvar):\n",
        "    eps = tf.random.normal(shape=z_mean.shape)\n",
        "    return eps * tf.exp(z_logvar * .5) + z_mean\n",
        "\n",
        "  def call(self, x):\n",
        "    z_mean, z_logvar = self.encode(x)\n",
        "    z = self.reparameterize(z_mean, z_logvar)\n",
        "    y = self.decode(z)\n",
        "    return y\n",
        "\n",
        "  def compute_loss(self, x, y):\n",
        "    z_mean, z_logvar = self.encode(x)\n",
        "    z = self.reparameterize(z_mean, z_logvar)\n",
        "    y_pred = self.decode(z)  # w/o activation, thus not `decode` function\n",
        "\n",
        "    l2 = tf.math.squared_difference(y, y_pred)\n",
        "    reconstruction_loss = tf.reduce_mean(l2)  # we use mean instead of sum so that loss values are comparable with other methods\n",
        "    \n",
        "    loss_z = -0.5 * tf.reduce_sum(1.0 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar), 1) \n",
        "    loss = tf.reduce_mean(loss_z) / np.prod(self.data_size)  # but since we used mean in reconstruction loss - this term has to be normalized accordingly\n",
        "    \n",
        "    return reconstruction_loss + loss\n",
        "    \n",
        "  @tf.function\n",
        "  def train_step(self, x, y):\n",
        "    \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "    This function computes the loss and gradients, and uses the latter to\n",
        "    update the model's parameters.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = self.compute_loss(x, y)\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "  def _fit(self, x, y, epochs, batch_size,\n",
        "           validation_data, callbacks):\n",
        "    \n",
        "    # finally we have to implement custom training loop\n",
        "\n",
        "    h = tf.keras.callbacks.History()\n",
        "    h.history['loss'] = []\n",
        "    h.history['val_loss'] = []\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "      start_time = time.time()\n",
        "      n_smpl = len(x)\n",
        "      idx = np.random.permutation(n_smpl)\n",
        "      \n",
        "      x_permuted, y_permuted = x[idx], y[idx]\n",
        "      n_batch = n_smpl // batch_size\n",
        "\n",
        "      loss = 0\n",
        "      for batch_idx in range(n_batch):\n",
        "        x_batch = x_permuted[batch_idx * batch_size : (batch_idx+1) * batch_size]\n",
        "        y_batch = y_permuted[batch_idx * batch_size : (batch_idx+1) * batch_size]\n",
        "\n",
        "        loss += self.train_step(x_batch, y_batch).numpy()  # actual training update\n",
        "      \n",
        "      h.history['loss'].append(loss/n_batch)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      mean_loss = tf.keras.metrics.Mean()\n",
        "      val_x, val_y = validation_data\n",
        "      n_smpl = len(val_x)\n",
        "      n_batch = n_smpl // batch_size\n",
        "      for batch_idx in range(n_batch):\n",
        "        x_batch = val_x[batch_idx * batch_size : (batch_idx+1) * batch_size]\n",
        "        y_batch = val_y[batch_idx * batch_size : (batch_idx+1) * batch_size]\n",
        "        loss = self.compute_loss(x_batch, y_batch)\n",
        "        mean_loss(loss)\n",
        "\n",
        "      loss_mean = mean_loss.result()\n",
        "\n",
        "      h.history['val_loss'].append(loss_mean.numpy())\n",
        "      IPython.display.clear_output(wait=False)\n",
        "      print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'\n",
        "            .format(epoch, loss_mean, end_time - start_time))\n",
        " \n",
        "      for callback in callbacks:\n",
        "        callback.on_epoch_end(epoch, logs=None)\n",
        "\n",
        "    return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG8wCrNUbbvB"
      },
      "source": [
        "vcae = VCAE((28,28), noise_rate=0., n_code=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix9M7rs8bbvE"
      },
      "source": [
        "vcae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-R1o_WbbvM"
      },
      "source": [
        "vcae.fit(train_images, 25, validation_data=(test_images, test_labels), lr=0.0009, batch_size=128) \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBFPvoh3bVbM"
      },
      "source": [
        "vcae.plot_hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ltegFux_Kg"
      },
      "source": [
        "Visualize reconstruciton:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRj0w-IdbVbP"
      },
      "source": [
        "hist = vcae.sample_history\n",
        "\n",
        "smpl_epochs = list(hist.keys())\n",
        "\n",
        "smpl_ims = [[hist[ep]['x'][:10], hist[ep]['y'][:10]] for ep in smpl_epochs]\n",
        "\n",
        "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "\n",
        "s=1\n",
        "fig = plt.figure(figsize=(s*nx, s*ny))\n",
        "\n",
        "m = mosaic(smpl_ims[0])\n",
        "\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    m = mosaic(smpl_ims[i])\n",
        "    imsh.set_data(m)\n",
        "\n",
        "    ttl = plt.title(f'after epoch {i}')\n",
        "\n",
        "    return imsh\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))\n",
        "\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPlUHfTWXbET"
      },
      "source": [
        "Let's look at the latent representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrvukYbxXciL"
      },
      "source": [
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "hist = vcae.sample_history\n",
        "\n",
        "smpl_epochs = list(hist.keys())\n",
        "\n",
        "z_res = [hist[ep]['z_unif'][:, :2] for ep in smpl_epochs]\n",
        "l_res = [hist[ep]['l_unif'] for ep in smpl_epochs]\n",
        "\n",
        "\n",
        "scat = plt.scatter(z_res[0][:,0], z_res[0][:,1], c=l_res[0], cmap=cm.rainbow)\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(-3, 3)\n",
        "\n",
        "legend1 = plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
        "plt.gca().add_artist(legend1)\n",
        "plt.gca().set_aspect('equal')\n",
        "ttl = plt.title(f'after epoch {0}')\n",
        "\n",
        "def animate(i):\n",
        "    z = z_res[i]\n",
        "    scat.set_offsets(z)\n",
        "    ttl = plt.title(f'after epoch {i}')\n",
        "    return scat\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))\n",
        "\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCFt4HVzcQs"
      },
      "source": [
        "And let's sample from the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4ArVRRXJI_"
      },
      "source": [
        "n = 15\n",
        "zs = np.meshgrid(norm.ppf(np.linspace(0.003, 0.997, n)),  # sampling proportionally to the density\n",
        "                 norm.ppf(np.linspace(0.003, 0.997, n)))\n",
        "zs = np.stack(zs, axis=-1)\n",
        "\n",
        "def fn(ae):\n",
        "  ims = ae.decode(zs.reshape((-1, 2))).numpy()\n",
        "  sh = list(ims.shape)\n",
        "  ims = ims.reshape([n, n]+sh[1:])\n",
        "  plt.figure(figsize=(n, n))\n",
        "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "vcae.run_on_trained(fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PLAxq3XIho_"
      },
      "source": [
        "# Visualization excercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_81bI_-MYTo3"
      },
      "source": [
        "Option 1: Sample 31x31 and overlay with data points.\n",
        "\n",
        "Option 2: Visulize data-point in latent space\n",
        "\n",
        "\n",
        "```\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = plt.gca()\n",
        "\n",
        "for x_i, y_i, im_i in zip(x_arr, y_arr, images):\n",
        "    im = OffsetImage(im_i, zoom=0.5)\n",
        "    ab = AnnotationBbox(im, (x_i, y_i), xycoords='data', frameon=False)\n",
        "\n",
        "    ax.add_artist(ab)\n",
        "    ax.update_datalim([(x_i, y_i)])\n",
        "    ax.autoscale()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgVvSoXa5vOk"
      },
      "source": [
        "# Exercise 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3DrSyps5wWv"
      },
      "source": [
        "## Option 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "642wgmpp50IC"
      },
      "source": [
        "Use VAE to explore Fashion MNIST\n",
        " 1. train model\n",
        " 2. explore latent representation\n",
        " 3. find outliers\n",
        " 4. sample from the latent distribution\n",
        " 5. overlay with data points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHPnzzd_6PLO"
      },
      "source": [
        "## Option 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxT_gT9Nw8b"
      },
      "source": [
        "\n",
        "Use CAE to explore molecule images dataset\n",
        "\n",
        " 1. train model\n",
        " 2. explore latent representation\n",
        " 3. find outliers\n",
        " 4. sample from the latent distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBeQArv39VmY"
      },
      "source": [
        "#zinc usage example:\n",
        "\n",
        "#download and unpack\n",
        "fname = 'named_in_cells_pics_w64.zip'\n",
        "zinc_path = 'ZINC_data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQb9d9l2OLkd"
      },
      "source": [
        "url = 'https://drive.google.com/u/0/uc?id=1PvSuoUH9Ag4eTavAQHwiadDWDYq0AZLB&export=download'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(fname, 'wb').write(r.content)\n",
        "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "    zip_ref.extractall(zinc_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7tLeEKUJa_h"
      },
      "source": [
        "def load_ims(path):\n",
        "  files = glob.glob(path+'/named_in_cells_pics_w64/*.png')\n",
        "\n",
        "  ims = [(np.asarray(Image.open(fname))/255.).astype(np.float32) for fname in files]\n",
        "\n",
        "  return np.array(ims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4a-M9CHOpiJ"
      },
      "source": [
        "zinc_ims = load_ims(zinc_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HJNLmBm2nng"
      },
      "source": [
        "zinc_ims = 1 - zinc_ims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBcU9FcQOvFL"
      },
      "source": [
        "print(zinc_ims.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR-YxTbLyF-Q"
      },
      "source": [
        "plt.imshow(zinc_ims[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVruUa0d7fMw"
      },
      "source": [
        "## Option 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmFRGd9r7lur"
      },
      "source": [
        "\n",
        "Use CAE to explore molecule text description (SMILE) dataset\n",
        "\n",
        " 1. train model\n",
        " 2. explore latent representation\n",
        " 4. sample from the latent distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDooxtlkuh6"
      },
      "source": [
        "import collections\n",
        "import pickle\n",
        "\n",
        "with open(zinc_path+'/id_zinc_smile.pckl', 'rb') as f:\n",
        "  id_to_zinc_smile_20 = pickle.load(f)\n",
        "smiles = [smile for c,smile in id_to_zinc_smile_20.values()]\n",
        "smiles_padded = [smile + ' ' * (25-len(smile)) for smile in smiles]\n",
        "\n",
        "smiles_all = ''.join(smiles_padded)\n",
        "\n",
        "def build_dataset(words):\n",
        "    count = collections.Counter(words).most_common()\n",
        "    dictionary = {}\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary\n",
        "\n",
        "dictionary, reverse_dictionary = build_dataset(smiles_all)\n",
        "\n",
        "smiles_as_int = np.array( [[dictionary[c] for c in smile] for smile in smiles_padded])\n",
        "print(smiles_as_int[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1_R6CnAy7-F"
      },
      "source": [
        "## Option 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQiRcufh7_wR"
      },
      "source": [
        "\n",
        "Use CAE to explore waveform dataset\n",
        "\n",
        "1. Build 1D convolutional autoencoder with 32 latent values\n",
        "2. Visualise evolution of each of the parameters along the track\n",
        "3. Visualise evolution of each of the parameters along the track averaged per bar (16 16th)\n",
        "4. Play reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiDNhz0hzRlc"
      },
      "source": [
        "import soundfile as sf "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U2TQzjaw6hT"
      },
      "source": [
        "fname = '01. Phaxe & Morten Granau - The Collective (Original Mix)_137bpm_G.flac'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRgk9Avrw6hf"
      },
      "source": [
        "url = #\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(fname, 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEa_agLYzSMY"
      },
      "source": [
        "path = fname                                                  \n",
        "data, samplerate = sf.read(path, dtype='float32')\n",
        "print(data.shape, samplerate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqjTIeon1nUN"
      },
      "source": [
        "#remove silence in the beginning\n",
        "data = data[19801:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FibEmpgrzprz"
      },
      "source": [
        "bpm = 137\n",
        "dt_beat = 60 / bpm  # sec\n",
        "samples_per_beat_f = dt_beat * samplerate\n",
        "samples_per_16th_f = samples_per_beat_f / 16\n",
        "\n",
        "samples_per_16th_i = int(samples_per_16th_f)\n",
        "\n",
        "print(samples_per_16th_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CYNIvtoz3ET"
      },
      "source": [
        "n_16th = int(len(data) / samples_per_16th_f)\n",
        "\n",
        "one_16th_chunks = [data[int(i*samples_per_16th_f): int(i*samples_per_16th_f) + samples_per_16th_i] for i in range(n_16th-1)]\n",
        "one_16th_chunks = np.array(one_16th_chunks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ittcsvdt2a6K"
      },
      "source": [
        "one_16th_chunks.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BGlI65K2juo"
      },
      "source": [
        "plt.plot(one_16th_chunks[::1000, :, 0].T);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs34FxZ94lCn"
      },
      "source": [
        "Let's play first 16 bar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z9--5Rx3ud9"
      },
      "source": [
        "samples_per_block = int(samples_per_16th_f * 16 * 4 * 16)\n",
        "\n",
        "Audio(data[:samples_per_block].T, rate=samplerate, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cwcUqm73K0v"
      },
      "source": [
        "# first kick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWCCpUqTzhHO"
      },
      "source": [
        "plt.plot(one_16th_chunks[16*4*16*2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uis5WTpI4wvF"
      },
      "source": [
        "Audio(one_16th_chunks[16*4*16*2].T, rate=samplerate, )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}