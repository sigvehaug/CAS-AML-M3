{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smjlo1x48N3t"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6H2W9QmdP0U"
      },
      "source": [
        "<p>\n",
        "CAS on Advanced Machine Learning <br>\n",
        "Data Science Lab, University of Bern, 2023<br>\n",
        "Prepared by Dr. Mykhailo Vladymyrov.\n",
        "\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVG7kpjf8T2D"
      },
      "source": [
        "# Libs and utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "GynY3qcttQPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjo8ffOptAcP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# set env var to allow duplicated lib\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRCK8pzztAcQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import einops as eo\n",
        "import pathlib as pl\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import collections  as mc\n",
        "from matplotlib import animation\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import entropy\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from time import time as timer\n",
        "#import umap\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Audio\n",
        "import IPython\n",
        "\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import sys\n",
        "is_colab = 'google.colab' in sys.modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlSvdkTItAcR"
      },
      "outputs": [],
      "source": [
        "# get mean and std of an array with numpy:\n",
        "def get_mean_std(x):\n",
        "    x_mean = np.mean(x)\n",
        "    x_std = np.std(x)\n",
        "    return x_mean, x_std\n",
        "\n",
        "# get min and max of an array with numpy:\n",
        "def get_min_max(x):\n",
        "    x_min = np.min(x)\n",
        "    x_max = np.max(x)\n",
        "    return x_min, x_max\n",
        "\n",
        "def is_iterable(obj):\n",
        "    try:\n",
        "        iter(obj)\n",
        "    except Exception:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def type_len(obj):\n",
        "    t = type(obj)\n",
        "    if is_iterable(obj):\n",
        "        sfx = f', shape: {obj.shape}' if t == np.ndarray else ''\n",
        "        print(f'type: {t}, len: {len(obj)}{sfx}')\n",
        "    else:\n",
        "        print(f'type: {t}, len: {len(obj)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N78bzRywtAcc"
      },
      "outputs": [],
      "source": [
        "def to_np_showable(pt_img):\n",
        "  np_im = pt_img.detach().cpu().numpy()\n",
        "  if len(np_im.shape) == 4:\n",
        "    np_im = np_im[0]\n",
        "\n",
        "  if np_im.shape[0] > 3:\n",
        "    np_im = np_im[-3:]\n",
        "\n",
        "  return (eo.rearrange(np_im, 'c h w -> h w c')/2+.5).clip(0., 1.)\n",
        "\n",
        "def plot_im(im, is_torch=True):\n",
        "  plt.imshow(to_np_showable(im) if is_torch else im, cmap='gray')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "def plot_im_samples(ds, n=5, is_torch=False):\n",
        "  fig, axs = plt.subplots(1, n, figsize=(16, n))\n",
        "  for i, image in enumerate(ds[:n]):\n",
        "      axs[i].imshow(to_np_showable(image) if is_torch else image, cmap='gray')\n",
        "      axs[i].set_axis_off()\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyC2UKsetAce"
      },
      "outputs": [],
      "source": [
        "# merging 2d matrix of images in 1 image\n",
        "def mosaic(mtr_of_ims):\n",
        "  ny = len(mtr_of_ims)\n",
        "  assert(ny != 0)\n",
        "\n",
        "  nx = len(mtr_of_ims[0])\n",
        "  assert(nx != 0)\n",
        "\n",
        "  im_sh = mtr_of_ims[0][0].shape\n",
        "\n",
        "  assert (2 <= len(im_sh) <= 3)\n",
        "  multichannel = len(im_sh) == 3\n",
        "\n",
        "  if multichannel:\n",
        "    h, w, c = im_sh\n",
        "  else:\n",
        "    h, w = im_sh\n",
        "\n",
        "  h_c = h * ny + 1 * (ny-1)\n",
        "  w_c = w * nx + 1 * (nx-1)\n",
        "\n",
        "  canv_sh = (h_c, w_c, c) if multichannel else (h_c, w_c)\n",
        "  canvas = np.ones(shape=canv_sh, dtype=np.float32)*0.5\n",
        "\n",
        "  for iy, row in enumerate(mtr_of_ims):\n",
        "    y_ofs = iy * (h + 1)\n",
        "    for ix, im in enumerate(row):\n",
        "      x_ofs = ix * (w + 1)\n",
        "      canvas[y_ofs:y_ofs + h, x_ofs:x_ofs + w] = im\n",
        "\n",
        "\n",
        "  return canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHjhiIzRtAcf"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')  # use first available GPU\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5NaXhkctAcg"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "print(f'device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3xcUuod8bUt"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWqy2uMffPA"
      },
      "source": [
        "Lets start with a simple, well understood mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSU2wVsXtAcj"
      },
      "outputs": [],
      "source": [
        "NOISE_RATE = 0.1\n",
        "N_SAMPLE = 32\n",
        "N_VIS_SAMPLE = 2\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSPzUDchtAck"
      },
      "outputs": [],
      "source": [
        "def collate_ae_dataset(samples):\n",
        "    \"\"\"\n",
        "    The function collates sampels into a batch, and creates noisy samples if DENOISING is True\n",
        "    for the denoising autoencoder.\n",
        "    \"\"\"\n",
        "    xs = [s[0] for s in samples]\n",
        "    ys = [s[1] for s in samples]\n",
        "    xs = torch.stack(xs)\n",
        "    ys = torch.concat(ys)\n",
        "\n",
        "    add_noise = NOISE_RATE > 0.\n",
        "    if add_noise:\n",
        "      sh = xs.shape\n",
        "      noise_mask = torch.bernoulli(torch.full(sh, NOISE_RATE))  # 0 (keep) or 1 (replace with noise)\n",
        "      sp_noise = torch.bernoulli(torch.full(sh, 0.5))-0.5  # -1 or 1\n",
        "\n",
        "      xns = xs * (1-noise_mask) + sp_noise * noise_mask\n",
        "\n",
        "      # sp = sp_noise\n",
        "    else:\n",
        "       xns = xs\n",
        "\n",
        "    return xns.to(device), xs.to(device), ys.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilgX4HUUtAcl"
      },
      "outputs": [],
      "source": [
        "m, s = 0.5, 1.\n",
        "# m, s = 0.5, 0.5\n",
        "#m, s = 0., 1.\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Pad(2), # to make images 32x32\n",
        "    transforms.Normalize((m,), (s,))\n",
        "])\n",
        "\n",
        "lable_transform = transforms.Compose([lambda x:torch.LongTensor([x])])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform, target_transform=lable_transform)\n",
        "valid_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform, target_transform=lable_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_ae_dataset, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_ae_dataset, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sZyi9IPtAcr"
      },
      "outputs": [],
      "source": [
        "for s in train_loader:\n",
        "  xns, xs, ys = s\n",
        "  print(xns.shape, xs.shape, ys.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIPvKCH9tAcs"
      },
      "outputs": [],
      "source": [
        "plot_im_samples(xns, is_torch=True)\n",
        "plot_im_samples(xs, is_torch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mMqQFO_tAcs"
      },
      "outputs": [],
      "source": [
        "# fill array of all preprocessed training samples, converted to numpy:\n",
        "train_images = []\n",
        "for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "    train_images.append(data.detach().cpu().numpy())\n",
        "\n",
        "train_images = np.concatenate(train_images, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmN0NguPtAc3"
      },
      "outputs": [],
      "source": [
        "print (\"train_images.shape = \", train_images.shape)\n",
        "print (\"train_images.dtype = \", train_images.dtype)\n",
        "print (\"train_images.mean/std() = \", get_mean_std(train_images))\n",
        "print (\"train_images.min/max() = \", get_min_max(train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uafPhG4EtAc4"
      },
      "outputs": [],
      "source": [
        "plt.hist(train_images.flatten(), bins=100, log=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta5guwIstAc4"
      },
      "outputs": [],
      "source": [
        "del train_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8kScAZTtAc5"
      },
      "source": [
        "We will also prepare a subsampled dataset from the validation set for the visualisation purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koccayybtAc5"
      },
      "outputs": [],
      "source": [
        "def get_samples(valid_loader):\n",
        "  # 1. get numpy array of all validation images:\n",
        "  val_images_noisy = []\n",
        "  val_images = []\n",
        "  val_labels = []\n",
        "\n",
        "  for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "      val_images_noisy.append(noisy_data.detach().cpu().numpy())\n",
        "      val_images.append(data.detach().cpu().numpy())\n",
        "      val_labels.append(target.detach().cpu().numpy())\n",
        "\n",
        "  val_images_noisy = np.concatenate(val_images_noisy, axis=0)\n",
        "  val_images = np.concatenate(val_images, axis=0)\n",
        "  val_labels = np.concatenate(val_labels, axis=0)\n",
        "\n",
        "  # 2. get numpy array of balanced validation samples for visualization:\n",
        "  sample_images_noisy = []\n",
        "  sample_images = []\n",
        "  sample_labels = []\n",
        "  single_el_idx = []  # indexes of single element per class for visualization\n",
        "\n",
        "  n_class = np.max(val_labels) + 1\n",
        "  for class_idx in range(n_class):\n",
        "    map_c = val_labels == class_idx\n",
        "\n",
        "    ims_c_noisy = val_images_noisy[map_c]\n",
        "    ims_c = val_images[map_c]\n",
        "\n",
        "    samples_idx = np.random.choice(len(ims_c), N_SAMPLE, replace=False)\n",
        "\n",
        "    ims_c_noisy_samples = ims_c_noisy[samples_idx]\n",
        "    ims_c_samples = ims_c[samples_idx]\n",
        "\n",
        "    sample_images_noisy.append(ims_c_noisy_samples)\n",
        "    sample_images.append(ims_c_samples)\n",
        "\n",
        "    sample_labels.append([class_idx]*N_SAMPLE)\n",
        "\n",
        "    start_idx = N_SAMPLE*class_idx\n",
        "    single_el_idx.extend([start_idx + i for i in range(min(N_VIS_SAMPLE, N_SAMPLE))])\n",
        "\n",
        "  sample_images_noisy = np.concatenate(sample_images_noisy, axis=0)\n",
        "  sample_images = np.concatenate(sample_images, axis=0)\n",
        "  sample_labels = np.concatenate(sample_labels, axis=0)\n",
        "  single_el_idx = np.array(single_el_idx)\n",
        "\n",
        "  samples = {\n",
        "      'images_noisy': sample_images_noisy,\n",
        "      'images': sample_images,\n",
        "      'labels': sample_labels,\n",
        "      'single_el_idx': single_el_idx\n",
        "\n",
        "  }\n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFJ_pQgrtAc6"
      },
      "outputs": [],
      "source": [
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYAEkBDKtAc6"
      },
      "outputs": [],
      "source": [
        "samples['images'].shape, samples['labels'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F94TFGHtAc6"
      },
      "outputs": [],
      "source": [
        "single_el_idx = samples['single_el_idx']\n",
        "plot_im_samples(samples['images_noisy'][single_el_idx, 0], n=20, is_torch=False)\n",
        "plot_im_samples(samples['images'][single_el_idx, 0], n=20, is_torch=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbR9emfD8d41"
      },
      "source": [
        "# Helper Autoencoder Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNM2dEZOh5EN"
      },
      "source": [
        "We will start from implementing an Autoencoder model base class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGs_gV5GtAc8"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_size, code_size):\n",
        "        self.input_size = list(input_size)  # shape of data sample\n",
        "        self.flat_data_size = np.prod(self.input_size)\n",
        "        self.hidden_size = 128\n",
        "\n",
        "        self.code_size = code_size  # code size\n",
        "\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(self.flat_data_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, self.code_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.code_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, self.flat_data_size),\n",
        "            nn.Tanh(),  # Think: why tanh?\n",
        "\n",
        "            nn.Unflatten(1, self.input_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_z=False):\n",
        "        encoded = self.encode(x)\n",
        "        decoded = self.decode(encoded)\n",
        "        return (decoded, encoded) if return_z else decoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)*1.1\n",
        "\n",
        "    def get_n_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def eval_on_samples(ae_model, epoch, samples):\n",
        "    # this is called on end of each training epoch\n",
        "    xns = samples['images_noisy']\n",
        "    xns = torch.tensor(xns, dtype=torch.float32).to(device)\n",
        "    #labels = samples['labels']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        yz = ae_model(xns, return_z=True)\n",
        "        yz = [el.detach().cpu().numpy() for el in yz]\n",
        "\n",
        "        y = yz[0]\n",
        "        z = yz[1:]\n",
        "\n",
        "    res = {'z': z, 'y': y, 'epoch': epoch}\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoITlVuntAc8"
      },
      "outputs": [],
      "source": [
        "def plot_hist(history, logscale=True):\n",
        "    \"\"\"\n",
        "    plot training loss\n",
        "    \"\"\"\n",
        "\n",
        "    loss = history['loss']\n",
        "    v_loss = history['val_loss']\n",
        "    epochs = history['epoch']\n",
        "\n",
        "    plot = plt.semilogy if logscale else plt.plot\n",
        "    plot(epochs, loss, label='training');\n",
        "    plot(epochs, v_loss, label='validation');\n",
        "    plt.legend()\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_samples(sample_history, samples, epoch_stride=5, fig_scale=1):\n",
        "    \"\"\"\n",
        "    Plots input, noisy samples (for DAE) and reconstruction.\n",
        "    Each `epoch_stride`-th epoch\n",
        "    \"\"\"\n",
        "\n",
        "    single_el_idx = samples['single_el_idx']\n",
        "    images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
        "    images = samples['images'][single_el_idx, 0]\n",
        "\n",
        "    last_epoch = np.max(list(sample_history.keys()))\n",
        "\n",
        "    for epoch_idx, hist_el in sample_history.items():\n",
        "      if epoch_idx % epoch_stride != 0 and epoch_idx != last_epoch:\n",
        "        continue\n",
        "\n",
        "      samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
        "\n",
        "      ny = len(samples_arr)\n",
        "      nx = len(samples_arr[0])\n",
        "\n",
        "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
        "      m = mosaic(samples_arr)\n",
        "\n",
        "      plt.title(f'after epoch {int(epoch_idx)}')\n",
        "      plt.imshow(m, cmap='gray', vmin=-.5, vmax=.5)\n",
        "      plt.tight_layout(pad=0.1, h_pad=0, w_pad=0)\n",
        "      plt.show()\n",
        "      plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwnutofatAc9"
      },
      "outputs": [],
      "source": [
        "def run_on_trained(model, root_dir, run_fn, ep=None, model_filename=None):\n",
        "    \"\"\"\n",
        "    Helper function to excecute any function on model in state after `ep` training epoch\n",
        "    \"\"\"\n",
        "\n",
        "    if model_filename is None:\n",
        "        if ep is not None:\n",
        "            model_filename = root_dir/f'model_{ep:03d}.pth'\n",
        "        else:\n",
        "            model_filename = list(root_dir.glob('*.pth'))[-1]  # last model state\n",
        "\n",
        "    model_dict = torch.load(model_filename)\n",
        "\n",
        "    model.load_state_dict(model_dict['model_state_dict'])\n",
        "\n",
        "    run_fn(model)\n",
        "\n",
        "def run_on_all_training_history(model, root_dir, run_fn, n_ep=None):\n",
        "    \"\"\"\n",
        "    Helper function to excecute any function on model state after each of the training epochs\n",
        "    \"\"\"\n",
        "    if n_ep is not None:\n",
        "        for ep in range(n_ep):\n",
        "            print(f'running on epoch {ep+1}/{n_ep}...')\n",
        "            run_on_trained(model, root_dir, run_fn, ep=ep)\n",
        "    else:\n",
        "        for model_filename in root_dir.glob('*.pth'):\n",
        "            print(f'running on checkpoint {model_filename}...')\n",
        "            run_on_trained(model, root_dir, run_fn, model_filename=model_filename)\n",
        "    print(f'done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evn49kSItAc9"
      },
      "outputs": [],
      "source": [
        "train_batch = next(iter(train_loader))\n",
        "xns, xs, ys = train_batch\n",
        "print('sample shapes:', xns.shape, xs.shape, ys.shape)\n",
        "in_size = xns.shape[1:]\n",
        "\n",
        "ae = AutoEncoder(input_size=in_size, code_size=10).to(device)\n",
        "y = ae(xns)\n",
        "print('output shape:', y.shape)\n",
        "plot_im_samples(xns, is_torch=True)\n",
        "plot_im_samples(y, is_torch=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = xns[0]# - y[1]\n",
        "d = y[0]# - y[1]\n",
        "\n",
        "im0 = x[0].detach().cpu().numpy()\n",
        "im1 = d[0].detach().cpu().numpy()\n",
        "\n",
        "# plt.imshow(im, cmap='gray', vmin=-1, vmax=1)\n",
        "bins = np.linspace(-1, 1, 100)\n",
        "plt.hist(im0.flatten(), bins, alpha=0.3);\n",
        "plt.hist(im1.flatten(), bins, alpha=0.3);\n"
      ],
      "metadata": {
        "id": "vVMY2jaA62tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4i9tkmotAc_"
      },
      "outputs": [],
      "source": [
        "ae.get_n_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUG1JVSCtAc_"
      },
      "source": [
        "# Simple Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSylMQQItAc_"
      },
      "source": [
        "First we train create an autoencoder with 5 latent variables for MNIST dataset.\n",
        "Each image is 28x28 pixels.\n",
        "We start from image data since it's easy to interpret and judge the reconstruction quality visually, but the very same applies to other data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMBFRdOttAdA"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 5\n",
        "NOISE_RATE = 0\n",
        "MODEL_NAME = 'ae_model'\n",
        "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfsArKpntAdA"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-lP40zxtAdC"
      },
      "source": [
        "Train the model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N1cGSNCtAdN"
      },
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 50\n",
        "LR = 0.0009\n",
        "\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# implement loss explicitly\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# train the model\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
        "for epoch_idx in pbar:\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(noisy_data)\n",
        "        loss_value = loss(output, data)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss_value.detach().cpu().item()\n",
        "    epoch_loss /= len(train_loader)\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['epoch'].append(epoch_idx)\n",
        "    # update progress bar\n",
        "\n",
        "    # evaluate on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "            output = model(noisy_data)\n",
        "            loss_value = loss(output, data)\n",
        "            val_loss += loss_value.detach().cpu().item()\n",
        "        val_loss /= len(valid_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "    # evaluate on samples\n",
        "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "    sample_history[epoch_idx] = sample_res\n",
        "\n",
        "    # save model weights\n",
        "    torch.save({\n",
        "                'epoch': epoch_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "                }, model_root/f'model_{epoch_idx:03d}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3ZYMHrtAdO"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1F-fhnitAdP"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuIjJna0tAdU"
      },
      "source": [
        "Let's visually compare network's output with the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0HxF-QktAdU"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_history[49]['z'][0].T.shape"
      ],
      "metadata": {
        "id": "BzL1wwdEHPfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in sample_history[49]['z'][0].T:\n",
        "  plt.hist(d, 100, alpha=0.3);"
      ],
      "metadata": {
        "id": "Co7xehyUGsCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-YdzZ2QDwLg"
      },
      "source": [
        "# Denoising Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15yh2Cfvh9RZ"
      },
      "source": [
        "In denoising AE we create noisy samples and ask the model to reconstruct original (clean) samples. Here 20% of pixels will be corrupted.\n",
        "This forces the model to learn more robust representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r45WCm8FtAdW"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 5\n",
        "NOISE_RATE = 0.2\n",
        "MODEL_NAME = 'dae_model'\n",
        "model = AutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZgXG8vQtAdW"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N6i-gXjtAdX"
      },
      "source": [
        "Train the model for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UKAEm9RtAdc"
      },
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 15\n",
        "LR = 0.0009\n",
        "\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# implement loss explicitly\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# train the model\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
        "for epoch_idx in pbar:\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(noisy_data)\n",
        "        loss_value = loss(output, data)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss_value.detach().cpu().item()\n",
        "    epoch_loss /= len(train_loader)\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['epoch'].append(epoch_idx)\n",
        "    # update progress bar\n",
        "\n",
        "    # evaluate on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "            output = model(noisy_data)\n",
        "            loss_value = loss(output, data)\n",
        "            val_loss += loss_value.detach().cpu().item()\n",
        "        val_loss /= len(valid_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "    # evaluate on samples\n",
        "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "    sample_history[epoch_idx] = sample_res\n",
        "\n",
        "    # save model weights\n",
        "    torch.save({\n",
        "                'epoch': epoch_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "                }, model_root/f'model_{epoch_idx:03d}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqyRCzactAdo"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgqjO5RPtAdp"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmD1KtcltAdr"
      },
      "source": [
        "Visualise evolution of reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs-PtmOAtAdr"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4saA2FcDFcMq"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZpzOK_OtsUu"
      },
      "source": [
        "Use autoencoder to find outliers:\n",
        "1. preform reconstruciton for test dataset\n",
        "2. evaluate reconstruction loss and plot distrubution ()\n",
        "3. visualise poorely reconstructed samples\n",
        "\n",
        "Extra/Homework:\n",
        "Perform 2D UMAP embedding of the raw data points and image encoding (variables in latent space) for test set, plot it and highlight outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu5h3HaT6aCN"
      },
      "source": [
        "# Convolutional autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf1wpycv6djD"
      },
      "source": [
        "Instead of fully connected layers we can use strided convolutional layers in encoder, and transposed convolutions in decoder.\n",
        "This model will have less parameters due to the weight sharing, thus easier to train.\n",
        "\n",
        "After upscaling the image size will be a bit bigger then original, so we also crop reconstruction to the input image size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTsTRZ-atAdu"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalAutoEncoder(AutoEncoder):\n",
        "    def __init__(self, input_size, code_size):\n",
        "        self.input_size = list(input_size)  # shape of data sample\n",
        "\n",
        "        self.hidden_size = 32*2*2\n",
        "\n",
        "        self.code_size = code_size  # code size\n",
        "\n",
        "        super(ConvolutionalAutoEncoder, self).__init__(input_size, code_size)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,   8, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(8,   8, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(8,  16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16, 16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, self.hidden_size//8), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Linear(self.hidden_size//8, self.code_size),\n",
        "            # nn.Tanh(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.code_size, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Unflatten(1, (32, 2, 2)),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.ConvTranspose2d(16,  8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.ConvTranspose2d(8,   8, 3, padding=1, output_padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(8, 1, 3, padding=1, stride=1), nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def decode(self, z):\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
        "        return reconstruction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUZaXBP5tAdu"
      },
      "source": [
        "Thus we can try to reduce sise of the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySMlIeMdtAdv"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 2\n",
        "NOISE_RATE = 0.2\n",
        "MODEL_NAME = 'cdae_model'\n",
        "model = ConvolutionalAutoEncoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xns = torch.tensor(samples['images_noisy']).to(device)\n",
        "print(xns.shape)\n",
        "zs = model.encode(xns)\n",
        "ys = model(xns)\n",
        "print(zs.shape)\n",
        "print(ys.shape)"
      ],
      "metadata": {
        "id": "0hkKRmFfyfZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIexAmnYtAdw"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdqOdtc-tAdw"
      },
      "outputs": [],
      "source": [
        "model.get_n_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAqBhNcAtAdw"
      },
      "source": [
        "Train the model for 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBP1oJqKtAdx"
      },
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 50\n",
        "LR = 0.0004\n",
        "\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# implement loss explicitly\n",
        "loss = nn.L1Loss()\n",
        "\n",
        "# train the model\n",
        "history = {'loss': [], 'val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
        "for epoch_idx in pbar:\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(noisy_data)\n",
        "        loss_value = loss(output, data)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss_value.detach().cpu().item()\n",
        "    epoch_loss /= len(train_loader)\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['epoch'].append(epoch_idx)\n",
        "    # update progress bar\n",
        "\n",
        "    # evaluate on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "            output = model(noisy_data)\n",
        "            loss_value = loss(output, data)\n",
        "            val_loss += loss_value.detach().cpu().item()\n",
        "        val_loss /= len(valid_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "    # evaluate on samples\n",
        "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "    sample_history[epoch_idx] = sample_res\n",
        "\n",
        "    # save model weights\n",
        "    torch.save({\n",
        "                'epoch': epoch_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "                }, model_root/f'model_{epoch_idx:03d}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccaQpXXptAdy"
      },
      "source": [
        "Plot loss function evolution during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYXjiKE1tAdy"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSvnZCWvtAd0"
      },
      "source": [
        "Visualise evolution of reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRwLeOzGtAd0"
      },
      "outputs": [],
      "source": [
        "plt.hist(samples['images'].flatten(), bins=100, log=True);\n",
        "for k in ['y', 'z']:\n",
        "    #print(sample_history[0][k].shape)\n",
        "    plt.hist(np.array(sample_history[0][k]).flatten(), bins=100, log=True);\n",
        "    plt.hist(np.array(sample_history[epoch_idx-1][k]).flatten(), bins=100, log=True);\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkLWD8yXtAd0"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVUXN5Jq7kx9"
      },
      "source": [
        "# Latent space\n",
        "\n",
        "So far we looked just on the output: the model sort of does the job. But what does it learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mVBP8b0t-Wl"
      },
      "source": [
        "First let's use animation to visualize reconstruction evolution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wzDeCVObTlI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "single_el_idx = samples['single_el_idx']\n",
        "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
        "images = samples['images'][single_el_idx, 0]\n",
        "\n",
        "smpl_ims = []\n",
        "for epoch_idx, hist_el in sample_history.items():\n",
        "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
        "    smpl_ims.append(samples_arr)\n",
        "\n",
        "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
        "\n",
        "s=1\n",
        "fig = plt.figure(figsize=(s*nx, s*ny))\n",
        "\n",
        "m = mosaic(smpl_ims[0])\n",
        "\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "# plot 0th epoch - 0th frame\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "# this function will be called to render each of the frames\n",
        "def animate(i):\n",
        "    m = mosaic(smpl_ims[i])\n",
        "    imsh.set_data(m)\n",
        "\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "\n",
        "    return imsh\n",
        "\n",
        "# create animation\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p25DtqevVw7q"
      },
      "outputs": [],
      "source": [
        "# display animation\n",
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qu6J9kGuEt9"
      },
      "source": [
        "And let's see evolution of the latent representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngMUudwdDpg5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "labels = samples['labels']\n",
        "epochs = sorted(sample_history.keys())\n",
        "z_res = [sample_history[ep]['z'][0] for ep in epochs]\n",
        "\n",
        "\n",
        "scat = plt.scatter(z_res[0][:,0], z_res[0][:,1], c=labels, cmap=cm.rainbow)\n",
        "plt.xlim(-0.1, 1.1)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "\n",
        "ax = plt.gca()\n",
        "legend1 = ax.legend(*scat.legend_elements(), title=\"digits\")\n",
        "ax.add_artist(legend1)\n",
        "ax.set_aspect('equal')\n",
        "ttl = plt.title(f'after epoch {0}')\n",
        "\n",
        "def animate(i):\n",
        "    z = z_res[i]\n",
        "    scat.set_offsets(z)\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "    return scat\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(z_res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-sYawGtV1le"
      },
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qArUdx6YHzD"
      },
      "source": [
        "# Sampling from latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syOBR27sn6vm"
      },
      "source": [
        "But we can also use the trained model to generate samples based on the latent representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLRUV7ad5P8p"
      },
      "outputs": [],
      "source": [
        "z0 = np.random.uniform(size=(25, CODE_SIZE))  # get 20 random points in 2D sampled from uniform distribution between 0 and 1\n",
        "z0_t = torch.tensor(z0, dtype=torch.float32).to(device)\n",
        "\n",
        "ims_all = []\n",
        "\n",
        "# this function will be called in saved model state after each training epoch\n",
        "def fn(ae):\n",
        "  with torch.no_grad():\n",
        "    ims = ae.decode(z0_t)\n",
        "    ims = ims.detach().cpu().numpy()\n",
        "    ims_all.append(ims)\n",
        "\n",
        "run_on_all_training_history(model, model_root, fn, n_ep=4)\n",
        "\n",
        "ims_all = np.array(ims_all)\n",
        "print(ims_all.shape)\n",
        "ims_all = ims_all[:, :, 0, :, :]  # remove channel dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdGfJcLItAeC"
      },
      "outputs": [],
      "source": [
        "plt.hist(ims_all[0].flatten(), bins=100, log=True);\n",
        "plt.hist(ims_all[-1].flatten(), bins=100, log=True, alpha=0.5);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edg10WV74629"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "fig = plt.figure(figsize=(20, 1.5))\n",
        "\n",
        "m = mosaic([ims_all[0]])\n",
        "\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    m = mosaic([ims_all[i]])\n",
        "    imsh.set_data(m)\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "    return imsh\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(ims_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy49wsQJV5Ky"
      },
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk1sA3cNFuLB"
      },
      "source": [
        "# Interpolation in latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsyVm5m_F3ng"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "zs = np.meshgrid(np.linspace(0, 1, n_samples),\n",
        "                 np.linspace(0, 1, n_samples))\n",
        "zs = np.stack(zs, axis=-1).reshape(-1, 2)\n",
        "zs_t = torch.tensor(zs, dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHivvVP0KaQd"
      },
      "outputs": [],
      "source": [
        "def fn(ae):\n",
        "  with torch.no_grad():\n",
        "    ims_t = ae.decode(zs_t)\n",
        "    ims = ims_t.detach().cpu().numpy()\n",
        "    ims = ims[:, 0, :, :]  # remove channel dimension\n",
        "\n",
        "  sh = list(ims.shape)\n",
        "  ims = ims.reshape([n_samples, n_samples]+sh[1:])\n",
        "  plt.figure(figsize=(10, 10))\n",
        "\n",
        "  # here image order is reversed along y axis in `ims` to match y axis direction on the plot\n",
        "  # (y axis goes from bottom to top, while pixel order - from top to bottom)\n",
        "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "run_on_trained(model, model_root, fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phTJlLGx3a5H"
      },
      "source": [
        "# Exercise 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYaGkilctAeF"
      },
      "source": [
        "Work in 2 groups:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXr6-NGf6OW4"
      },
      "source": [
        "\n",
        "Option 1. Compare distribution in latent space for noise rate 0, 0.2, 0.6, 0.8.\n",
        "\n",
        "Option 2. Compare reconstruction depending on size of latent space: 2, 4, 8, 16, 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp-ZrvjUtAeG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBT5XbLztAeH"
      },
      "source": [
        "# Exercise 3: Writing latent Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A85koGGYtAeK"
      },
      "source": [
        "\n",
        "In this exercise, we will\n",
        "1. modify the CNN to be fully convolutional, with 4 downscaling layers (x16), 16 channels\n",
        "2. save dataset of latent states, see code below for example.\n",
        "\n",
        "When usuing VAE - save the Z nean and log std - separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iS4ZCW4tAeK"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_pckl(obj, fname):\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "def load_pckl(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPCKd8__tAeM"
      },
      "outputs": [],
      "source": [
        "#load latest model checkpoint:\n",
        "run_on_trained(model, model_root, lambda m:None, ep=3)\n",
        "\n",
        "# dataset for latent DDM, file list:\n",
        "data_files = {\"train\": [], \"test\": []}\n",
        "\n",
        "ds_root = pl.Path('data')\n",
        "ds_root.mkdir(exist_ok=True)\n",
        "\n",
        "# generate latent values per blocks of samples:\n",
        "block_sz = 5000\n",
        "for ds, sfx in zip([valid_dataset, train_dataset], ['test', 'train']):\n",
        "  x = ds.data.numpy().reshape(-1, 1, 28, 28)\n",
        "  x = x/255.*2-1\n",
        "  l = ds.targets.numpy()\n",
        "\n",
        "  for i in range(0, len(x), block_sz):\n",
        "    x_b = x[i:i+block_sz]\n",
        "    l_b = l[i:i+block_sz]\n",
        "\n",
        "    x_b_t = torch.tensor(x_b, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z_t = model.encode(x_b_t)\n",
        "        z_b = z_t.detach().cpu().numpy()\n",
        "\n",
        "    # save latent values and their shape and labels to a dataframe:\n",
        "    data_d = {\n",
        "    'z': [zi.flatten() for zi in z_b],\n",
        "    'shape': [zi.shape for zi in z_b],\n",
        "    'label': l_b\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data=data_d)\n",
        "\n",
        "    # save dataframe to a file and reference dictioanry:\n",
        "    fname = ds_root/f'df_z_{sfx}_{i}.pckl'\n",
        "    save_pckl(df, fname)\n",
        "    data_files[sfx].append(str(fname))\n",
        "\n",
        "save_pckl(data_files, ds_root/'data_files.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBIJwlWRtAeN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJELUTmKtAeO"
      },
      "outputs": [],
      "source": [
        "# now we can load the dataset:\n",
        "data_files = load_pckl(ds_root/'data_files.pkl')\n",
        "lds = load_dataset('pandas', data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BprdQfcEtAeU"
      },
      "outputs": [],
      "source": [
        "# convert to torch dataset:\n",
        "tds = lds.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5szuM5XtAeU"
      },
      "outputs": [],
      "source": [
        "tds['train']['z'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_tWAYpah3aG"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnBr9bvW39zT"
      },
      "source": [
        "While the previous model learn to seaparate subpopulations in the latent space, there remains significant overlap & unpopulated regions.\n",
        "\n",
        "Varitional AE puts additional constraints on the distribution in the latent space and perform variational inference.\n",
        "\n",
        "(see pptx for details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kshKMIOwwD9W"
      },
      "source": [
        "Here the encoder will predict `2 x n_code` values: means and logarithm of variance for each sample. Since these values live in $R^2$ - no activation function is used in last layer of the encoder.\n",
        "\n",
        "Then for reconstruction we will sample from this distribuition with a reparametrisation trick.\n",
        "\n",
        "The tecnically complex part - is to implement the custom loss function and training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaKbgRDetAeW"
      },
      "outputs": [],
      "source": [
        "class VariationalConvolutionalAutoencoder(AutoEncoder):\n",
        "    def __init__(self, input_size, code_size):\n",
        "        super(VariationalConvolutionalAutoencoder, self).__init__(input_size, code_size)\n",
        "\n",
        "        # nn.LeakyReLU(negative_slope=0.3)\n",
        "        self.input_size = list(input_size)  # shape of data sample\n",
        "        self.npix = np.prod(self.input_size)\n",
        "\n",
        "        self.hidden_size = 64*1\n",
        "\n",
        "        self.code_size = code_size  # code size\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,   16, 3, padding=1, stride=1), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16,  16, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(16,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(32,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(32,  32, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Conv2d(32,  64, 3, padding=1, stride=2), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Flatten(),\n",
        "\n",
        "            nn.Linear(self.hidden_size, 64), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            # we want values in all R, thus no activation function is applied. self.n_code values for mean + self.n_code for log(variance)\n",
        "            nn.Linear(64, self.code_size * 2),\n",
        "\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.code_size, 64), nn.LeakyReLU(negative_slope=0.3),\n",
        "            nn.Linear(64, self.hidden_size), nn.LeakyReLU(negative_slope=0.3),\n",
        "\n",
        "            nn.Unflatten(1, (64, 1, 1)),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 32, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 16, 3, padding=1, output_padding=1, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(16, 1, 3, padding=1, stride=1), nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            # samle from standard normal distribution\n",
        "            eps = torch.randn((100, self.code_size))\n",
        "        return self.decode(eps)\n",
        "\n",
        "    def encode(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z_mean, z_logvar = torch.split(z, split_size_or_sections=2, dim=1)\n",
        "        return z_mean, z_logvar\n",
        "\n",
        "    def reparameterize(self, z_mean, z_logvar):\n",
        "        # reaparametrization trick: to sample z from N(mean, std):\n",
        "        # z = mean + std * eps, where eps sampled from N(0, 1)\n",
        "        eps = torch.randn_like(z_mean)\n",
        "        z_std = torch.exp(z_logvar * .5)\n",
        "        return eps * z_std + z_mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction = reconstruction[:, :, 2:-2, 2:-2]\n",
        "        return reconstruction\n",
        "\n",
        "    def forward(self, x, return_z=False):\n",
        "        z_mean, z_logvar = self.encode(x)\n",
        "        z = self.reparameterize(z_mean, z_logvar)\n",
        "        reconstruction = self.decode(z)\n",
        "        return (reconstruction, z_mean, z_logvar) if return_z else reconstruction\n",
        "\n",
        "    def forward_and_KL_loss(self, x, y):\n",
        "        reconstruction, z_mean, z_logvar = self(x, return_z=True)\n",
        "\n",
        "        # tf impl:\n",
        "        # loss_z_kl = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + tf.square(z_mean) -1.0 - z_logvar, axis=1)  # KL divergence from N(0, 1) to N(z_mean, tf.exp(z_logvar * .5))\n",
        "        # loss_z_kl = tf.reduce_mean(loss_z_kl) / np.prod(self.data_size)  # but since we used mean in reconstruction loss - this term has to be normalized accordingly\n",
        "\n",
        "        # pytorch impl:\n",
        "        loss_z_kl = 0.5 * torch.sum(torch.exp(z_logvar) + torch.square(z_mean) -1.0 - z_logvar, dim=1)\n",
        "        loss_z_kl = torch.mean(loss_z_kl) / self.npix\n",
        "\n",
        "        return reconstruction, loss_z_kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHvSydQKtAeY"
      },
      "outputs": [],
      "source": [
        "CODE_SIZE = 2 # 50\n",
        "NOISE_RATE = 0.\n",
        "MODEL_NAME = 'vcae_model'\n",
        "model = VariationalConvolutionalAutoencoder(input_size=in_size, code_size=CODE_SIZE).to(device)\n",
        "\n",
        "samples = get_samples(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6ngCGuHtAed"
      },
      "outputs": [],
      "source": [
        "xns = torch.tensor(samples['images_noisy']).to(device)\n",
        "print(xns.shape)\n",
        "ys = model(xns)\n",
        "ys.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is6wGSR4tAee"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09QZTchYtAej"
      },
      "outputs": [],
      "source": [
        "model.get_n_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzquUs8AtAek"
      },
      "outputs": [],
      "source": [
        "# train the autoencoder model, for N_EPOCHS epochs,\n",
        "# save history of loss values for training and validation sets,\n",
        "# history of validation samples evolution, and model weights history,\n",
        "\n",
        "N_EPOCHS = 4 # 20\n",
        "LR = 0.0009\n",
        "\n",
        "\n",
        "model_root = pl.Path(MODEL_NAME)\n",
        "model_root.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# implement loss explicitly\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# train the model\n",
        "history = {'loss': [], 'val_loss': [], 'rec_loss': [], 'rec_val_loss': [], 'kl_loss': [], 'kl_val_loss': [], 'epoch': []}\n",
        "sample_history = {}\n",
        "\n",
        "pbar = tqdm.tqdm(range(N_EPOCHS), postfix=f'epoch 0/{N_EPOCHS}')\n",
        "for epoch_idx in pbar:\n",
        "    epoch_loss = 0\n",
        "    epoch_rec_loss = 0\n",
        "    epoch_kl_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (noisy_data, data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        #output = model(noisy_data)\n",
        "        output, kl_loss = model.forward_and_KL_loss(noisy_data, data)\n",
        "        rec_loss = loss(output, data)\n",
        "        loss_value = rec_loss + kl_loss\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss_value.detach().cpu().item()\n",
        "        epoch_rec_loss += rec_loss.detach().cpu().item()\n",
        "        epoch_kl_loss += kl_loss.detach().cpu().item()\n",
        "\n",
        "    n_elements = len(train_loader)\n",
        "\n",
        "    epoch_loss /= n_elements\n",
        "    epoch_rec_loss /= n_elements\n",
        "    epoch_kl_loss /= n_elements\n",
        "\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['rec_loss'].append(epoch_rec_loss)\n",
        "    history['kl_loss'].append(epoch_kl_loss)\n",
        "\n",
        "    history['epoch'].append(epoch_idx)\n",
        "    # update progress bar\n",
        "\n",
        "    # evaluate on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        val_rec_loss = 0\n",
        "        val_kl_loss = 0\n",
        "\n",
        "        for batch_idx, (noisy_data, data, target) in enumerate(valid_loader):\n",
        "            #output = model(noisy_data)\n",
        "            output, kl_loss = model.forward_and_KL_loss(noisy_data, data)\n",
        "            rec_loss = loss(output, data)\n",
        "            loss_value = rec_loss + kl_loss\n",
        "\n",
        "            val_loss += loss_value.detach().cpu().item()\n",
        "            val_rec_loss += rec_loss.detach().cpu().item()\n",
        "            val_kl_loss += kl_loss.detach().cpu().item()\n",
        "\n",
        "        val_loss /= len(valid_loader)\n",
        "        val_rec_loss /= len(valid_loader)\n",
        "        val_kl_loss /= len(valid_loader)\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['rec_val_loss'].append(val_rec_loss)\n",
        "        history['kl_val_loss'].append(val_kl_loss)\n",
        "\n",
        "    pbar.set_postfix({'epoch': f'{epoch_idx+1}/{N_EPOCHS}', 'loss':f'{epoch_loss:.4f}', 'val_loss':f'{val_loss:.4f}'})\n",
        "    # evaluate on samples\n",
        "    sample_res = eval_on_samples(model, epoch_idx, samples=samples)\n",
        "    sample_history[epoch_idx] = sample_res\n",
        "\n",
        "    # save model weights\n",
        "    torch.save({\n",
        "                'epoch': epoch_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "                }, model_root/f'model_{epoch_idx:03d}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix9M7rs8bbvE"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBFPvoh3bVbM"
      },
      "outputs": [],
      "source": [
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "invB2CcVtAem"
      },
      "outputs": [],
      "source": [
        "plot_samples(sample_history, samples=samples, epoch_stride=5, fig_scale=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ltegFux_Kg"
      },
      "source": [
        "Visualize reconstruciton:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRj0w-IdbVbP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "single_el_idx = samples['single_el_idx']\n",
        "images_noisy = samples['images_noisy'][single_el_idx, 0]\n",
        "images = samples['images'][single_el_idx, 0]\n",
        "\n",
        "smpl_ims = []\n",
        "for epoch_idx, hist_el in sample_history.items():\n",
        "    samples_arr = [images_noisy, hist_el['y'][single_el_idx, 0], images]\n",
        "    smpl_ims.append(samples_arr)\n",
        "\n",
        "ny, nx = len(smpl_ims[0]), len(smpl_ims[0][0])\n",
        "\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above animations use JavaScript\n",
        "\n",
        "s=1\n",
        "fig = plt.figure(figsize=(s*nx, s*ny))\n",
        "\n",
        "m = mosaic(smpl_ims[0])\n",
        "\n",
        "ttl = plt.title(f'after epoch {int(0)}')\n",
        "# plot 0th epoch - 0th frame\n",
        "imsh = plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "\n",
        "# this function will be called to render each of the frames\n",
        "def animate(i):\n",
        "    m = mosaic(smpl_ims[i])\n",
        "    imsh.set_data(m)\n",
        "\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "\n",
        "    return imsh\n",
        "\n",
        "# create animation\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(smpl_ims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj6X_wLbVeae"
      },
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPlUHfTWXbET"
      },
      "source": [
        "Let's look at the latent representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rdrCY9ftAe0"
      },
      "outputs": [],
      "source": [
        "sample_history[0]['z'][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrvukYbxXciL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above, uses JavaScript\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "\n",
        "smpl_epochs = list(sample_history.keys())\n",
        "\n",
        "z_res_mean = [sample_history[ep]['z'][0] for ep in smpl_epochs]\n",
        "z_logvar = [sample_history[ep]['z'][1] for ep in smpl_epochs]\n",
        "z_res_std = np.exp(np.array(z_logvar) * .5)\n",
        "\n",
        "\n",
        "l_res = samples['labels']\n",
        "l_col = [cm.rainbow(l_i/10) for l_i in l_res]\n",
        "\n",
        "# error bars:\n",
        "def get_lines(m, s):\n",
        "  l = []\n",
        "  for (x, y), (sx, sy) in zip(m, s):\n",
        "    l.append([(x-sx, y), (x+sx, y)]) # h\n",
        "    l.append([(x, y-sy), (x, y+sy)]) # w\n",
        "  return np.array(l)\n",
        "\n",
        "lines = get_lines(z_res_mean[0], z_res_std[0])\n",
        "lc = mc.LineCollection(lines, color=l_col, linewidths=2, alpha=0.3)\n",
        "fig.gca().add_collection(lc)\n",
        "\n",
        "scat = plt.scatter(z_res_mean[0][:,0], z_res_mean[0][:,1], c=l_res, cmap=cm.rainbow)\n",
        "#scat_err = plt.errorbar(z_res_mean[0][:,0], z_res_mean[0][:,1], xerr=z_res_std[0][:,0], yerr=z_res_std[0][:,1], fmt=\"o\")\n",
        "\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-4, 4)\n",
        "\n",
        "legend1 = plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
        "plt.gca().add_artist(legend1)\n",
        "plt.gca().set_aspect('equal')\n",
        "ttl = plt.title(f'after epoch {0}')\n",
        "\n",
        "def animate(i):\n",
        "    z = z_res_mean[i]\n",
        "    scat.set_offsets(z)\n",
        "\n",
        "    lines = get_lines(z_res_mean[i], z_res_std[i])\n",
        "    lc.set_segments(lines)\n",
        "\n",
        "    ttl.set_text(f'after epoch {i}')\n",
        "    return scat, lc\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(z_res_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOoFVobIVXVD"
      },
      "outputs": [],
      "source": [
        "ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCFt4HVzcQs"
      },
      "source": [
        "And let's sample from the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87T1eLW9tAe2"
      },
      "outputs": [],
      "source": [
        "zs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m4ArVRRXJI_"
      },
      "outputs": [],
      "source": [
        "n = 20\n",
        "zs = np.meshgrid(norm.ppf(np.linspace(0.0001, 0.9999, n)),  # sampling proportionally to the density\n",
        "                 norm.ppf(np.linspace(0.0001, 0.9999, n)))\n",
        "# zs = np.meshgrid(np.linspace(-4, 4, n),  # sampling linearly\n",
        "#                  np.linspace(-4, 4, n))\n",
        "zs = np.stack(zs, axis=-1)\n",
        "zs = zs.reshape((-1, 2))\n",
        "\n",
        "zs_t = torch.tensor(zs, dtype=torch.float32).to(device)\n",
        "\n",
        "def fn(ae):\n",
        "  with torch.no_grad():\n",
        "    ims_t = ae.decode(zs_t)\n",
        "    ims = ims_t.detach().cpu().numpy()\n",
        "    ims = ims[:, 0, :, :]  # remove channel dimension\n",
        "\n",
        "  sh = list(ims.shape)\n",
        "  ims = ims.reshape([n, n]+sh[1:])\n",
        "  plt.figure(figsize=(n, n))\n",
        "  plt.imshow(mosaic(ims[::-1]), vmin=0, vmax=1, cmap='gray')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "run_on_trained(model, model_root, fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnLTJeN1fYz6"
      },
      "outputs": [],
      "source": [
        "def fn(ae):\n",
        "  # x = samples['images_noisy']\n",
        "  # l = samples['labels']\n",
        "  # ds = valid_dataset\n",
        "  ds = train_dataset\n",
        "  x = ds.data.numpy().reshape(-1, 1, 28, 28)\n",
        "  x = x/255.*2-1\n",
        "  l = ds.targets.numpy()\n",
        "  x_t = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "  z_m, z_s = ae.encode(x_t)\n",
        "  z_m, z_s = [el.detach().cpu().numpy() for el in [z_m, z_s]]\n",
        "\n",
        "  z_res_std = np.exp(np.array(z_s) * .5)\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  scat = plt.scatter(*z_m.T, c=l, s=2, cmap='jet')\n",
        "  plt.gca().legend(*scat.legend_elements(), title=\"digits\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 2), ncols=2, nrows=1)\n",
        "  ax[0].hist(z_m.flatten(), bins=100);\n",
        "  ax[1].hist(z_res_std.flatten(), bins=100);\n",
        "  ax[0].set_title('mean')\n",
        "  ax[1].set_title('std')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "run_on_trained(model, model_root, fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PLAxq3XIho_"
      },
      "source": [
        "# Visualization excercise (homework)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_81bI_-MYTo3"
      },
      "source": [
        "Option 1: Sample 31x31 and overlay with data points.\n",
        "\n",
        "Option 2: Visulize data-point in latent space\n",
        "\n",
        "\n",
        "```\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = plt.gca()\n",
        "\n",
        "# given the image sample array `image_arr` and the pair of coordinate arrays\n",
        "# of the latent representation z0_arr, z1_arr:\n",
        "for z0_i, z1_i, im_i in zip(z0_arr, z1_arr, image_arr):\n",
        "    im = OffsetImage(im_i, zoom=0.5)\n",
        "    ab = AnnotationBbox(im, (z0_i, z1_i), xycoords='data', frameon=False)\n",
        "\n",
        "    ax.add_artist(ab)\n",
        "    ax.update_datalim([(z0_i, z1_i)])\n",
        "    ax.autoscale()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgVvSoXa5vOk"
      },
      "source": [
        "# Exercise 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3DrSyps5wWv"
      },
      "source": [
        "## Option 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "642wgmpp50IC"
      },
      "source": [
        "Use VAE to explore Fashion MNIST\n",
        " 1. train model\n",
        " 2. explore latent representation\n",
        " 3. find outliers\n",
        " 4. sample from the latent distribution\n",
        " 5. overlay with data points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHPnzzd_6PLO"
      },
      "source": [
        "## Option 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxT_gT9Nw8b"
      },
      "source": [
        "\n",
        "Use CAE to explore molecule images dataset\n",
        "\n",
        " 1. train model. Which size of the latent dimension would make sense?\n",
        " 2. explore latent representation\n",
        " 3. find outliers\n",
        " 4. sample from the latent distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBeQArv39VmY"
      },
      "outputs": [],
      "source": [
        "#download and unpack\n",
        "fname = 'named_in_cells_pics_w64.zip'\n",
        "zinc_path = 'ZINC_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQb9d9l2OLkd"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/sigvehaug/CAS-AML-M3/blob/main/named_in_cells_pics_w64.zip?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(fname, 'wb').write(r.content)\n",
        "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "    zip_ref.extractall(zinc_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7tLeEKUJa_h"
      },
      "outputs": [],
      "source": [
        "def load_ims(path):\n",
        "  files = glob.glob(path+'/named_in_cells_pics_w64/*.png')\n",
        "\n",
        "  ims = [(np.asarray(Image.open(fname))/255.).astype(np.float32) for fname in files]\n",
        "\n",
        "  return np.array(ims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4a-M9CHOpiJ"
      },
      "outputs": [],
      "source": [
        "zinc_ims = load_ims(zinc_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HJNLmBm2nng"
      },
      "outputs": [],
      "source": [
        "zinc_ims = 1 - zinc_ims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBcU9FcQOvFL"
      },
      "outputs": [],
      "source": [
        "print(zinc_ims.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR-YxTbLyF-Q"
      },
      "outputs": [],
      "source": [
        "plt.imshow(zinc_ims[7000], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVruUa0d7fMw"
      },
      "source": [
        "## Option 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmFRGd9r7lur"
      },
      "source": [
        "\n",
        "Use CAE to explore molecule text description (SMILE) dataset.\n",
        "\n",
        "Input will be sequence of 25 character id-s.  \n",
        "Instead of manually encodeing input in 1-hot encoding, you can use\n",
        "`Embedding(input_dim=n_dict, output_dim=dim_embedding)` as first encoder layer which accepts class id-s as inputs .\n",
        "\n",
        "The output - probability of each possible character (`n_dict = len(dictionary)` channels).\n",
        "The model is trained in classicication framework - with cross-entropy loss.\n",
        "\n",
        "Fully connected and convolutional network can be used.\n",
        "In case of convolutional to implement effectively 1D convolutions you need to specify hight of convolutional kernel as 1, e.g.: `Conv2D(16,  (1, 3), padding='same', activation=LeakyReLU(alpha=0.1), kernel_initializer='he_normal')`. Note that input to the convolutional operations should be still 4D: n_batch x hight x width x n_channel\n",
        "\n",
        "Suggested code size = 32\n",
        "\n",
        " 1. train model\n",
        " 2. explore latent representation\n",
        " 4. sample from the latent distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSZlvRM47p4I"
      },
      "outputs": [],
      "source": [
        "#download and unpack\n",
        "fname = 'named_in_cells_pics_w64.zip'\n",
        "zinc_path = 'ZINC_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBRCyQ2Q7p4T"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/sigvehaug/CAS-AML-M3/blob/main/named_in_cells_pics_w64.zip?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(fname, 'wb').write(r.content)\n",
        "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "    zip_ref.extractall(zinc_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBtfajFD7thR"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pickle\n",
        "\n",
        "with open(zinc_path+'/id_zinc_smile.pckl', 'rb') as f:\n",
        "  id_to_zinc_smile_20 = pickle.load(f)\n",
        "smiles = [smile for c,smile in id_to_zinc_smile_20.values()]\n",
        "\n",
        "print(smiles[0], smiles[20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hTbyr9Z70s5"
      },
      "outputs": [],
      "source": [
        "plt.hist([len(s) for s in smiles], 25); # length histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLDooxtlkuh6"
      },
      "outputs": [],
      "source": [
        "smiles_padded = [smile + ' ' * (25-len(smile)) for smile in smiles]  # pad all with spaces to have same length\n",
        "\n",
        "smiles_all = ''.join(smiles_padded)\n",
        "\n",
        "def build_dataset(symbols):\n",
        "    count = collections.Counter(symbols).most_common()  # count symbols characters and sort by occurance\n",
        "    dictionary = {}\n",
        "    for symbol, _ in count:\n",
        "        dictionary[symbol] = len(dictionary)  # make dictionary for encoding each symbol with symbol ID.\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary\n",
        "\n",
        "dictionary, reverse_dictionary = build_dataset(smiles_all)\n",
        "\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXl36U1RlRoM"
      },
      "outputs": [],
      "source": [
        "len(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji1h01g88FKY"
      },
      "outputs": [],
      "source": [
        "smiles_as_int = np.array( [[dictionary[c] for c in smile] for smile in smiles_padded])  # make dataset\n",
        "print(smiles_as_int[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1_R6CnAy7-F"
      },
      "source": [
        "## Option 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQiRcufh7_wR"
      },
      "source": [
        "\n",
        "Use CAE to explore waveform dataset.\n",
        "To implement effectively 1D convolutions you need to specify hight of convolutional kernel as 1, e.g.: `Conv2D(16,  (1, 3), padding='same', activation=LeakyReLU(alpha=0.1), kernel_initializer='he_normal')`. Note that input to the convolutional operations should be still 4D: n_batch x hight x width x n_channel\n",
        "\n",
        "1. Build 1D convolutional autoencoder with 32 latent values\n",
        "2. Visualise evolution of each of the parameters along the track\n",
        "3. Visualise evolution of each of the parameters along the track averaged per bar (16 16th)\n",
        "4. Play reconstruction.\n",
        "5. To improve quality try to use Gated activation units https://arxiv.org/pdf/1609.03499.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiDNhz0hzRlc"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U2TQzjaw6hT"
      },
      "outputs": [],
      "source": [
        "fname = '01. Phaxe & Morten Granau - The Collective (Original Mix)_137bpm_G.flac'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRgk9Avrw6hf"
      },
      "outputs": [],
      "source": [
        "url = #\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(fname, 'wb').write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEa_agLYzSMY"
      },
      "outputs": [],
      "source": [
        "path = fname\n",
        "data, samplerate = sf.read(path, dtype='float32')\n",
        "print(data.shape, samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqjTIeon1nUN"
      },
      "outputs": [],
      "source": [
        "#remove silence in the beginning\n",
        "data = data[19801:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FibEmpgrzprz"
      },
      "outputs": [],
      "source": [
        "bpm = 137  # 137 beats per minute, 4/4 tact\n",
        "dt_beat = 60 / bpm  # sec\n",
        "samples_per_beat_f = dt_beat * samplerate\n",
        "samples_per_16th_f = samples_per_beat_f / 4\n",
        "\n",
        "samples_per_16th_i = int(samples_per_16th_f)\n",
        "\n",
        "print(samples_per_16th_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CYNIvtoz3ET"
      },
      "outputs": [],
      "source": [
        "n_16th = int(len(data) / samples_per_16th_f)\n",
        "\n",
        "one_16th_chunks = [data[int(i*samples_per_16th_f): int(i*samples_per_16th_f) + samples_per_16th_i] for i in range(n_16th-1)]\n",
        "one_16th_chunks = np.array(one_16th_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ittcsvdt2a6K"
      },
      "outputs": [],
      "source": [
        "one_16th_chunks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BGlI65K2juo"
      },
      "outputs": [],
      "source": [
        "plt.plot(one_16th_chunks[::1000, :, 0].T);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs34FxZ94lCn"
      },
      "source": [
        "Let's play first 16 bar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z9--5Rx3ud9"
      },
      "outputs": [],
      "source": [
        "samples_per_block = int(samples_per_16th_f * 16 * 4 * 16)\n",
        "\n",
        "Audio(data[:samples_per_block].T, rate=samplerate, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cwcUqm73K0v"
      },
      "outputs": [],
      "source": [
        "# first kick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWCCpUqTzhHO"
      },
      "outputs": [],
      "source": [
        "plt.plot(one_16th_chunks[16*4*16*2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uis5WTpI4wvF"
      },
      "outputs": [],
      "source": [
        "Audio(one_16th_chunks[16*4*16*2].T, rate=samplerate, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGdgxx9LngGC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "smjlo1x48N3t",
        "TVG7kpjf8T2D",
        "cbR9emfD8d41",
        "9QH3J3Z-jTb2",
        "7-YdzZ2QDwLg",
        "4saA2FcDFcMq",
        "tu5h3HaT6aCN",
        "AVUXN5Jq7kx9",
        "9qArUdx6YHzD",
        "Tk1sA3cNFuLB",
        "phTJlLGx3a5H",
        "8_tWAYpah3aG",
        "9PLAxq3XIho_",
        "lgVvSoXa5vOk",
        "G3DrSyps5wWv",
        "kHPnzzd_6PLO",
        "YVruUa0d7fMw",
        "G1_R6CnAy7-F"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}